---
title: 文本压缩算法的对比和选择
date: 2024-10-04T15:30:53.321Z
categories:
  - zhihu
tags:
  - zhihu
  - 写代码的
origin_url: //zhuanlan.zhihu.com/p/28035800
---
本文将粗略介绍主要的数据压缩算法，以及最新针对 Web 文本资源的 zStd 和 Brotli 算法的设计要点。为 Web 业务应用如何使用它们替换传统 gzip 提供些参考。最后演示下达成文本有损压缩的尝试。

在数据压缩领域里，文本压缩的历史最久，从 Morse 到 Huffman 和算术编码 (Arithmetic coding)，再到基于字典和上下文的压缩算法。各种算法不断改进，从通用算法，到现在更具针对性的算法，结合应用场景的垂直化的趋势越来越明显。所以**在选择或者评价压缩算法，一定要结合实际应用场景加以考虑，包括字符集、内容的大小、压缩及解压的性能、以及各端 (特别是浏览器和应用) 支持情况**。

## 数据压缩算法

一套完整的压缩算法，实际以下几个部分：

![](https://picx.zhimg.com/v2-c19f6aeccf300684ea6073e68f7ff61d_b.png)

其中除编码外的三项目的都是找到一个适于编码的表示方法，而编码则是以简化的方法进行输出。最典型的建模方法是基于字符的概率统计，而基于上下文的建模方法 (Context Modeling) 则是从文本内容出发，它们追求的目标都是让字符的出现概率越不平均越好。转换方法是最具代表性的是基于词典的转换，比如庞大的 LZ 族系。Huffman 和算术编码则是常见的编码方法。

因为语言本身的特性，基于上下文的建模方法 (Context Modeling，如 PPM \* 系列算法) 可以得到更好的压缩比，但却由于它的性能问题却很难普及。当前比较流行的压缩算法中其突破的核心只有两个:

* ANS (FSE 是它的一个实现): Facebook zStd, Apple 的 lzfse 等。
* Context Modeling + LZ77 (编码是 Huffman): Brotli, bz2 也应用了其中的 BWT 算法。

\


下图为六种算法的压缩比测试的结果，分别针对一本英文小说，一本中文小说，和一份较小 (4KB+) 的中文混合的 JSON 数据。

![](https://pic1.zhimg.com/v2-31d52dc20166b8186249850384fd1886_b.png)

&#x20;*\* 其中 PPM 是 Context Modeling 的代表算法。*&#x20;

可以看到算法对字符集 (中文与英文) 和大小都是敏感的，表现各不相同。

## 算法思想的简单概括

**Huffman 编码**受到了 Morse 编码的影响，背后的思想就是将最高概率出现的字母以最短的编码表示。比如英文中字母 e 出现概率为 12%，字母 z 的出现概率还不到 1%（数据来源:[Letter Frequency](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Letter_frequency)）。 **算术编码以及区间编码**\*\*，它们是利用字符概率分布，将字符组合转变为概率的层次划分，最终转换一个固定的数字 (算术编码和区间编码最大差别就在于一个使用小数，另一个使用整数)。可以对应下图考虑下 AAAA，以及 AAB 的编码输出 (在 0-1 的轴上找到一个数字来表示。)。

![](https://picx.zhimg.com/v2-a34353b9fb47f9f9e76dbb71d822de51_b.png)

参考维基上的说明:[算术编码](https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/zh-hans/%25E7%25AE%2597%25E6%259C%25AF%25E7%25BC%2596%25E7%25A0%2581)。 上面这两类算法一直霸占着算法编码领域，各自拥有大量的变形算法。

从实用效果上，算术编码的压缩比一般要好于 Huffman。但后者的性能则要优于前者，两者都有自适应的算法，不必依赖全文进行概率统计，但算术编码还是需要更大的计算量。

**ANS**是前两类编码算法战争的终结者。它在 2014 年被提出来，随后很快就得到了大量应用。本质上属于算术编码，但它成功地找到了一个用**近似概率**表示的表格，将原来的概率计算转换为查表。所以它是一个达到 Huffman 编码效率的算术编码方法。[FSE(Finite State Entropy)](https://link.zhihu.com/?target=https%3A//github.com/Cyan4973/FiniteStateEntropy)是 ANS 最为著名的实现。有兴趣进一步了解，可以看[这里](https://link.zhihu.com/?target=http%3A//www.ezcodesample.com/abs/abs_article.html)。

之前提过**建模方法**，要追求字符出现概率的不平均。比如**动态马尔可夫压缩 (DMC, dynamic Markov coding)**。仍以英为例，全文来看，字母 e 出现概率最高，但是在首字母这种状态下，字母 t 的出现概率可以接近 17%。就是字母在是否首字母的两种状态下是有两种概率分布的：

![](https://picx.zhimg.com/v2-f1b595e77f4c74871249ddda8efc8279_b.png)

** *\* 非首母下，各字母的出现概率有可能还有变化。* **

**基于上下文的建模**，可以想象下成语填空或者诗词填空，其中有一个条件概率的问题。在下图中，如果单个字符看，我们只能从整体汉字的概率分布来考虑 \`\`\`人\`\`\` 后面的字。如果从词的角度发出，后面确实可能会出现几个概率较大的单字，这是一阶上下文。再进一步，如果之前出现过单字'**华**'，这是二阶上下文 (2nd order)，后面文字出现的概率又会发生变化。如果再往前取一个单字又是'**中**'，那么不单是后面单字'**民**' 的概率极高，而且再往后有三个字的出现概率也是奇高的。在随后编码时，我们就可以为它找出一个最短的表示。

![](https://picx.zhimg.com/v2-0a7796360f74b086a585d38008a737f9_b.png)

实际应用不可能让算法先要理解文字，绝大部分情况也不可能为此先建一个语料库，即使有语料库，其处理性能仍然会是很大的问题。其实我们也不需要得到极为精确的相关性，只要快速掌握到一定的模式就足够了。好像学生根本不需要做大规模的训练，就能很快注意到老师的口头禅或者常用语，进而学得有模有样。所以可行的算法只需要基于一部分内容的上下文进行预测，这就是 **PPM（部分匹配预测，prediction by partial match）** 以及各种演进版本。

**转换 (Transform)**里词典方法很好理解，就不介绍了，已经是应用最为广泛技术。而文本压缩基本目标是无损压缩，所以 \*\* 去冗余 \*\* 这项我们最后再谈。

## 内容字符集与大小的影响

我们思考一个问题：为什么各种算法对内容字符集和大小的效果不同？

**字符集的差异**很好理解，由文字的信息熵所决定。有很多人在连续很多年都讨论过这个问题，也有非常精确的汉字信息熵的评价。单纯从当前编码的角度来看，就是作为中文基础单元的单字数远比英文中基本字母多得多。大的字符集很自然就使得字符间的概率差异较小，所以此时 Huffman 和算术编码这类依赖于概率统计的算法对于中文压缩都远不及对英文压缩的效果。

回头再看下上面算法对比结果。当纯英文时，算术编码压缩比优于 Huffman。遇到大字符集的中文时，基本和 Huffman 一样了。这两个算法在这个测试场景没有拉来明显差距，可以动手换个二进数据再对比一下。

如果遇到中英文混合，且内容较小的 JSON 数据时，算术编码就歇菜了。

**内容大小的影响**是来自于算法本身的'**学习成本**'。从统计的角度看，内容大小代表了样本的多寡，会直接影响统计结果。以基于概率统计的算法为例，如果极少字符，有利于编码。但同时没有足够的数据量，字符间没法形成概率分布的差异，又不利于编码。两者共同决定了最终的压缩比。

在测试数据里**基于上下文建模的 PPM**表现明显优于 Huffman 和算术编码。说明虽然仅仅在局部理解字符出现的相关性，就已经能够很好地优化效果了。即使是小数据，也远高于前面两类的效果。

到这里，请思考一个问题：

&#x20;**什么方法能降低压缩算法的学习成本？**&#x20;

那么我们需要先定义:

* 字符集是什么？它的规模多大？字符的概率分布如何？
* 内容会多大？效率和费用的问题，我们不讨论了。这个时候 \*\* 转换 \*\* 就能发挥最大的功效了，特别预设字典类的算法。

\


下面是一更为具体的问题，也可以练习一下:

&#x20;*如果我们要设计一个极短文本（如 100 字以内）的压缩算法，什么会是最为效的算法？*&#x20;

**

## Brotli 与 zStd 的对比和选择

在 Web 应用场景下，压缩算法要追求的是更小且更快（对于流式数据，还有吞吐量的要求，另有算法应对），算法主要在压缩比和性能之间寻求平衡。目前页面上的各种文本资源主要还是使用 gzip 压缩，自 zStd 和 Brotli 推出后，我们该如何选择呢？ 如果你只想了解一下两者的选择，可以跳过下面关于两者对比的一节。

## 两个算法的对比

Brotli 算法可以理解为 LZ\* + Context Modeling + Huffman 编码。这是一个极为完整的无损压缩算法，三个部分全涉及到了。

它的 compression level 分为 11 级，一般测试而言它的 level 5 可以达到 gzip 9 的效果。主要特色包括:

* \* 词典的 Sliding Window 扩充到 16MB
* \* 针对 6 国常用词及 HTML\&JS 常用关键词的静态 (预置) 词典
* \* 基于二阶上下文建模 (2nd order context modeling) 注意 context modeling 的性能很弱，如果对压缩性能要求不高（如本地 PC 压缩），可以把 level 调到 6 以上测试下。

\


zStd 是基于 FSE (ANS 的一个实现)，针对 MB 级别以上的数据，最为有效。 但是对于小数据，它特别提供一个预置词典的方法（回顾下前面关于小数据压缩的问题）。这个方法需要通过对目标数据进行训练从而生成这个词典。步骤如下:

> 1\) 词典训练\
> zstd --train FullPathToTrainingSet/\* -o dictionaryName\
> 2\) 压缩\
> zstd -D dictionaryName FILE\
> 3\) 解压\
> zstd -D dictionaryName --decompress FILE.zst

这种方法的一个局限就是应用的范围，需要既要有相对稳定的内容，又要有客户端的支持。

## 如何选择

如果两个可以同时选择时:

&#x20;**较大的文本数据，选择 zStd。较小的数据，选择 Brotli。** \*

大和小的边界需要根据内容测试来定义。500KB 左右，zStd 的表现基本会超过 Brotli。

&#x20;**不要使用默认值对比。**&#x20;

Brotli 默认的 compression level 是它的最大值，即 11。而 zstd 默认是 3, gzip 默认为 6。

以下是使用 UC 头条的 33 条数据，大小在 3\~26KB，包括 HTML 和 JSON 数据，在 Mac Pro 上做的对比测试。其中 zStd 没有使用词典。

针对性能，直接使用工具对每个文件调用一次，最后加总时间的方法进行测试，并不是工具批次处理功能，所以存在一定的优化空间，特别是 Brotli 这种需要加载预置词典的算法。

从完成全部文件压缩所用时间上看，zstd/brotli 对比 gzip 的耗时都有一定的增加。zstd-10 和 brotli-6 基本接近:

![](https://pica.zhimg.com/v2-da699d6324e4ac7e7ce4698a393c412c_b.png)

但从压缩比上看，brotli-6 的效果就要优于 zstd-10 了:

![](https://pica.zhimg.com/v2-a192f078245e6cd6a87dc8b7d2df047e_b.png)

&#x20;**最后需要考虑的是，应用场景下各端的支持情况。这里不做赘述！**&#x20;

## 算法选择需要考虑的因素

所以写到这里，得出的结论只有一个：**选择一个文本压缩算法最有效的方式是实验**。

不要轻信别人的测试结果，如果不知道为什么？麻烦回到最上方，重读一遍。只有通过实验，才能得出一个更为有效的算法以及参数的选择。

* **压缩比** 确定出目标内容，并按涉及的字符集和大小，按组合和梯度做好测试。
* **压缩性能 & 解压性能** 执行压缩和解压的设备各是什么？性能带来的延迟影响有多大？
* **内容动态性** 即目标内容变化频率。比如实时压缩，且吞吐量较大，你就要评估下 LZ4 算法。其它情况要用好缓存和 CDN。
* **各端支持的情况 **特别是客户端、浏览器上的支持情况。
* **专利费用 **比如算术编码是有专利费用的，而区间编码则是免费的。

\


## 文本的有损压缩？

开篇也提到了针对场景的压缩算法研究是目前的趋势，特别是找到更有效的建模方法。传统意义上理解数据压缩，包括文本压缩都应当是无损的。但其实也有一种场景是需要有损压缩的。

比如早期电报，惜字如金。在现在的场景下，内容的概要处理就是有损压缩的一个典型需求。内容的大爆发之后就有内容的进一步管理和挖掘的问题。内容的概要处理一是方便展示，二是方便检索和归类。

借助现在 NLP 领域的成果，我们可以很轻松地对句子结构和成分进行分析。可以在句子中定位出介词短语 (PP)，量词短语（QP），动词短语（VP），又或者名词短语（NP）等。基于这个结果，可以定位出句子的主要成分，然后把修饰成分作为冗余去掉（\*\* 去冗余 \*\*）。

以下面的句子为例:

&#x20;**从乐视危机、易到事件，再到资金冻结、机构撤资，最近半年，乐视就像一部跌宕起伏的电视剧，作为乐视的创始人，贾跃亭一次又一次被推上风口浪尖。**&#x20;

![](https://pic3.zhimg.com/v2-eb072db9424db96708f7938df5114756_b.png)

我们去掉修饰短语，使用图中标出的名词及动词短语，就很容易得到核心成份: \`\`\`贾跃亭被推上风口浪尖。\`\`\` 示例使用 R 及 Standford CoreNLP 包在 Mac OS 下完成，源码在[GitHub](https://link.zhihu.com/?target=https%3A//github.com/HorkyChen/LossyTextCompression)上。 虽然目前这种处理的性能还是未能达到产品化要求，但相关的应用已经近在眼前。

因为非压缩领域的专业人员，本文是一段时间学习的总结。一定有错误和理解不透的地方，欢迎指正！

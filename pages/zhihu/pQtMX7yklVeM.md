---
title: 全面梳理：准确率,精确率,召回率,查准率,查全率,假阳性,真阳性,PRC,ROC,AUC,F1
date: 2024-10-04T15:30:53.384Z
categories:
  - zhihu
tags:
  - zhihu
  - 写代码的
origin_url: //zhuanlan.zhihu.com/p/34079183
---
二分类问题的结果有四种：

![](https://pica.zhimg.com/v2-129d7b63bb994ca0a708b67d3fbe34ee_b.jpg)

逻辑在于，**你的预测是 positive-1 和 negative-0，true 和 false 描述你本次预测的对错**

true positive-TP：预测为 1，预测正确即实际 1

false positive-FP：预测为 1，预测错误即实际 0

true negative-TN：预测为 0，预测正确即实际 0

false negative-FN：预测为 0，预测错误即实际 1

【混淆矩阵】

直观呈现以上四种情况的样本数

【准确率】accuracy

正确分类的样本 / 总样本：(TP+TN)/(ALL)

在不平衡分类问题中难以准确度量：比如 98% 的正样本只需全部预测为正即可获得 98% 准确率

【精确率】【查准率】precision

TP/(TP+FP)：在你预测为 1 的样本中实际为 1 的概率

查准率在检索系统中：检出的**相关**文献与**检出的全部文献**的百分比，衡量**检索的信噪比**

【召回率】【查全率】recall

TP/(TP+FN)：在实际为 1 的样本中你预测为 1 的概率

查全率在检索系统中：检出的**相关**文献与**全部相关**文献的百分比，衡量**检索的覆盖率**

实际的二分类中，positive-1 标签可以代表健康也可以代表生病，但一般作为**positive-1**的指标指的是**你更关注的样本表现**，比如 “是垃圾邮件”“是阳性肿瘤”“将要发生地震”。

因此在肿瘤判断和地震预测等场景：

**要求模型有更高的【召回率】recall**，是个地震你就都得给我揪出来不能放过

在垃圾邮件判断等场景：

&#x20;**要求模型有更高的【精确率】precision，** 你给我放进回收站里的可都得确定是垃圾，千万不能有正常邮件啊

【ROC】

常被用来评价一个二值分类器的优劣

![](https://pica.zhimg.com/v2-4247ac9516f8595d2ce779b3c4fb1f22_b.jpg)

ROC 曲线的横坐标为 false positive rate（FPR）：**FP/(FP+TN)**

假阳性率，即实际无病，但根据筛检被判为有病的百分比。

在实际为 0 的样本中你预测为 1 的概率

纵坐标为 true positive rate（TPR）：**TP/(TP+FN)**

真阳性率，即实际有病，但根据筛检被判为有病的百分比。

在实际为 1 的样本中你预测为 1 的概率，此处即【召回率】【查全率】recall

接下来我们考虑 ROC 曲线图中的四个点和一条线。

第一个点，(0,1)，即 FPR=0,TPR=1，这意味着**无病的没有被误判，有病的都全部检测到**，这是一个完美的分类器，它将所有的样本都正确分类。

第二个点，(1,0)，即 FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。

第三个点，(0,0)，即 FPR=TPR=0，即 FP（false positive）=TP（true positive）=0，没病的没有被误判但有病的全都没被检测到，即全部选 0

类似的，第四个点（1,1），分类器实际上预测所有的样本都为 1。

经过以上的分析可得到：ROC 曲线越接近左上角，该分类器的性能越好。

【ROC 是如何画出来的】

分类器有概率输出，50% 常被作为阈值点，但基于不同的场景，可以通过控制概率输出的阈值来改变预测的标签，这样不同的阈值会得到不同的 FPR 和 TPR。

从 0%-100% 之间选取任意细度的阈值分别获得 FPR 和 TPR，对应在图中，得到的 ROC 曲线，阈值的细度控制了曲线的阶梯程度或平滑程度。

一个没有过拟合的二分类器的 ROC 应该是梯度均匀的，如图紫线

![](https://picx.zhimg.com/v2-06d06a5d5d6f0c03953f835936e8a1f1_b.jpg)

此图为 PRC， precision recall curve，原理类似

ROC 曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC 曲线能够保持不变。而 Precision-Recall 曲线会变化剧烈，故 ROC 经常被使用。

![](https://picx.zhimg.com/v2-78c4052e3aaa6a9ca2c64f5ccc4ece61_b.jpg)

【AUC】

AUC（Area Under Curve）被定义为 ROC 曲线下的面积，完全随机的二分类器的 AUC 为 0.5，虽然在不同的阈值下有不同的 FPR 和 TPR，但相对面积更大，更靠近左上角的曲线代表着一个更加稳健的二分类器。

同时针对每一个分类器的 ROC 曲线，又能找到一个最佳的概率切分点使得自己关注的指标达到最佳水平。

【AUC 的排序本质】

大部分分类器的输出是概率输出，如果要计算准确率，需要先把概率转化成类别，就需要手动设置一个阈值，而这个超参数的确定会对优化指标的计算产生过于敏感的影响

AUC 从 Mann–Whitney U statistic 的角度来解释：随机从标签为 1 和标签为 0 的样本集中分别随机选择两个样本，同时分类器会输出两样本为 1 的概率，那么我们认为分类器对**“标签 1 样本的预测概率> 对标签 0 样本的预测概率 ” 的概率**等价于 AUC。

因而 AUC 反应的是分类器对样本的排序能力，这样也可以理解 AUC 对不平衡样本不敏感的原因了。

【作为优化目标的各类指标】

最常用的分类器优化及评价指标是 AUC 和 logloss，最主要的原因是：不同于 accuracy，precision 等，这两个指标不需要将概率输出转化为类别，而是可以直接使用概率进行计算。

顺便贴上 logloss 的公式

![](https://pica.zhimg.com/v2-0bf9aa6d06d307a88fbc90e1414d8afc_b.jpg)

* N：样本数
* M：类别数，比如上面的多类别例子，M 就为 4
* yij：第 i 个样本属于分类 j 时为为 1，否则为 0
* pij：第 i 个样本被预测为第 j 类的概率

【F1】

![](https://pic4.zhimg.com/v2-3e447f481ef83aae4cb511779924c461_b.jpg)

F1 兼顾了分类模型的准确率和召回率，可以看作是模型准确率和召回率的**调和平均数**，最大值是 1，最小值是 0。

额外补充【AUC 为优化目标的模型融合手段**rank\_avg**】：

在拍拍贷风控比赛中，印象中一个前排队伍基于 AUC 的排序本质，使用 rank\_avg 融合了最后的几个基础模型。

![](https://pic3.zhimg.com/v2-505a3c0cbd58abe6a960cfd2fa36470c_b.jpg)

rank\_avg 这种融合方法适合排序评估指标，比如 auc 之类的

其中 weight\_i 为该模型权重，权重为 1 表示平均融合

rank\_i 表示样本的升序排名 ，也就是越靠前的样本融合后也越靠前

能较快的利用排名融合多个模型之间的差异，而不用去加权样本的概率值融合

贴一段源码：

```text
#三模型的概率输出
xgb_7844 = pd.read_csv ('xgb_7844.csv')
svm_771 = pd.read_csv ('svm_771.csv')
xgb_787 = pd.read_csv ('xgb_787.csv')

#score 概率变为排名
xgb_7844.score = xgb_7844.score.rank ()
svm_771.score = svm_771.score.rank ()
xgb_787.score = xgb_787.score.rank ()

# 排名加权融合的结果丧失了概率指义，但 AUC 的计算不用关系绝对大小，只关心相对大小
pred = 0.7*xgb_787.score + 0.2*xgb_7844.score + 0.1*svm_771.score

#AUC 的计算
auc = int (roc_auc_score (val.target.values,pred.values)*10000)
```

![](https://pic3.zhimg.com/v2-650f0ed7962d2ed9a2d724a4b84d2350_b.jpg)

M 为正类样本的数目，N 为负类样本的数目，rank 为分类器给出的排名。

可以发现整个计算过程中连直接的概率输出值都不需要，仅关心相对排名，所以只要保证 submit 的那一组输出的 rank 是有意义的即可，并不一定需要必须输出概率。

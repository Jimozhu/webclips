---
title: "Mastering LangChain Your Ultimate Guide To Getting Started"
date: 2023-05-23 13:53:53
categories: [other]
tags: []
origin_url: https://zilliz.com/blog/langchain-ultimate-guide-getting-started?utm_source=twitter&utm_medium=social&utm_term=zilliz
---
Have you seen the parrot + chain emoji popping up around AI lately? Those are LangChain’s signature emojis. LangChain is an AI Agent tool that adds functionality to large language models (LLMs) like GPT. In addition, it includes functionality such as token management and context management. For this getting started tutorial, we look at two primary examples of LangChain usage. First, how to query GPT. Second, how to query a document with a [Colab notebook available here](https://colab.research.google.com/drive/1KiPHx1wxfYylc3fqMP1hVsUB4yYtzQGn?usp=sharing).

In this tutorial we cover:

*   What is LangChain?
*   How Can You Run LangChain Queries?
*   Query GPT
*   Query a Document
*   Introduction to LangChain Summary

What is LangChain?
------------------

LangChain is a framework for building applications that leverage LLMs. It allows you to quickly build with the [CVP Framework](https://zilliz.com/blog/ChatGPT-VectorDB-Prompt-as-code). The two core functionalities that LangChain offers LLMs are 1) to be data-aware and 2) to be agentic. Data-awareness is the ability to incorporate outside data sources into an LLM application. Agency is the ability to use other tools.

As with many LLM tools, LangChain’s default LLM is OpenAI’s GPT and you need an [API key from OpenAI](https://platform.openai.com/docs/api-reference) to use it. In addition, LangChain works with both Python and JavaScript. In this tutorial, you will learn how it works using Python examples. You can install the Python library through `pip` by running `pip install langchain`.

How Can You Run LangChain Queries?
----------------------------------

One of the primary uses for LangChain is to query some text data. You can use it to query documents, vector stores, or to smooth your interactions with GPT, much like LlamaIndex. In this tutorial, we cover a simple example of how to interact with GPT using LangChain and query a document for semantic meaning using LangChain with a vector store.

### Query GPT

Most people’s familiarity with GPT comes from chatting with ChatGPT. ChatGPT is OpenAI’s flagship interface for interacting with GPT. However, if you want to interact with GPT programmatically, you need a query interface like LangChain. LangChain provides a range of query interfaces for GPT, from simple one-question prompts to few shot learning via context.

In this example, we’ll look at how to use LangChain to chain together questions using a prompt template. There are a few Python libraries you need to install first. We can install them with # `pip install langchain openai python-dotenv tiktoken`. I use `python-dotenv` because I manage my environment variables in a `.env` file, but you can use whatever method you want to get your OpenAI API key loaded.

With our OpenAI API key ready, we must load up our LangChain tools. We need the `PromptTemplate` and `LLMChain` imports from `langchain` and the `OpenAI` import from `langchain.llms`. We use OpenAI’s text model, `text-davinci-003` for this example. Next, we create a template to query GPT with. The template that we create below tells GPT to answer the given questions one at a time. First, we create a string representing input variables within brackets, similar to how `f-strings` work.

import os
from dotenv import load\_dotenv
import openai
load\_dotenv()
openai.api\_key = os.getenv("OPENAI\_API\_KEY")


from langchain import PromptTemplate, LLMChain
from langchain.llms import OpenAI
davinci = OpenAI(model\_name="text-davinci-003")
multi\_template = """Answer the following questions one at a time.


Questions:
{questions}


Answers:
"""

  
    
    
  

Next, we use the `PromptTemplate` object to create a template from the string with specified input variables. With our prompt template ready, we can create an LLM “chain” by passing in the prompt and the chosen LLM. Now it’s time to create the questions. Once we have the questions we want to ask, we `run` the LLM chain with the questions passed in to get our answers.

llm\_chain = LLMChain(
   prompt=long\_prompt,
   llm=davinci
)
qs\_str = (
   "Which NFL team won the Super Bowl in the 2010 season?\\n" +
   "If I am 6 ft 4 inches, how tall am I in centimeters?\\n" +
   "Who was the 12th person on the moon?" +
   "How many eyes does a blade of grass have?"
)
print(llm\_chain.run(qs\_str))

The image below shows the expected results from our example questions.

[![](https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/45ac067b.png)](https://assets.zilliz.com/Langchain_blog_1_results_08dedb25d4.png)

Results from example questions.

### Query a Document

One of the areas for improvement of GPT and LLMs at large is that they are only trained on data available at their time of training. This means that over time, they lose context and accuracy. Much like the CVP framework, LangChain provides a way to remediate this problem with vector databases. Many vector databases are available; for this example, we use Milvus because of our familiarity with it.

To demonstrate LangChain’s ability to inject up-to-date knowledge into your LLM application and the ability to do a [semantic search](https://zilliz.com/glossary/semantic-search), we cover how to query a document. We use a transcript from the State of the Union address for this example. You can download the transcript [here](https://github.com/hwchase17/langchain/blob/master/docs/modules/state_of_the_union.txt) and find the [Colab notebook here](https://colab.research.google.com/drive/1KiPHx1wxfYylc3fqMP1hVsUB4yYtzQGn?usp=sharing). To get the libraries you need for this part of the tutorial, run `pip install langchain openai milvus pymilvus python-dotenv tiktoken`.

As with the example of chaining questions together, we start by loading our OpenAI API key and the LLM. Then, we spin up a vector database using [Milvus Lite](https://milvus.io/docs/milvus_lite.md), which allows us to run Milvus directly in our notebook.

import os
from dotenv import load\_dotenv
import openai
load\_dotenv()
openai.api\_key = os.getenv("OPENAI\_API\_KEY")
from langchain.llms import OpenAI
davinci = OpenAI(model\_name="text-davinci-003")
from milvus import default\_server
default\_server.start()

Now we are ready to get into the specifics of querying a document. There are a lot of imports from LangChain this time. We need the Open AI Embeddings, the character text splitter, the Milvus integration, the text loader, and the retrieval Q/A chain.

The first thing we do is set up a loader and load the text file. In this case, I have it stored in the same folder as this notebook under `state_of_the_union.txt.` Next, we split up the text and store it as a set of docs in LangChain. Then, we can set up our vector database. In this case, we create a Milvus collection from the documents we just ingested via the `TextLoader` and `CharacterTextSplitter`. We also pass in the OpenAI embeddings as the set of text vector embeddings.

With our vector database loaded, we can use the `RetrievalQA` object to query the documents via a vector database. We use the chain type `stuff " and pass in OpenAI as our LLM and the Milvus vector database as a retriever. Then, we can create a query, such as “What did the president say about Ketanji Brown Jackson?” and`run\` the query. Finally, we should shut down our vector database for a clean close.

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text\_splitter import CharacterTextSplitter
from langchain.vectorstores import Milvus
from langchain.document\_loaders import TextLoader
from langchain.chains import RetrievalQA


loader = TextLoader('./state\_of\_the\_union.txt')
documents = loader.load()
text\_splitter = CharacterTextSplitter(chunk\_size=1000, chunk\_overlap=0)
docs = text\_splitter.split\_documents(documents)
embeddings = OpenAIEmbeddings()
vector\_db = Milvus.from\_documents(
   docs,
   embeddings,
   connection\_args={"host": "127.0.0.1", "port": default\_server.listen\_port},
)
qa = RetrievalQA.from\_chain\_type(llm=OpenAI(), chain\_type="stuff", retriever=vector\_db.as\_retriever())
query = "What did the president say about Ketanji Brown Jackson?"
qa.run(query)
default\_server.stop()

The image below shows what an expected response could look like. We should get a response like “The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, …”

[![](https://note-2019-images.oss-cn-hangzhou.aliyuncs.com/3b8acf90.png)](https://assets.zilliz.com/Langchain_blog_results2_v2_ec22fde211.png)Query results

Summary of an Introduction to LangChain
---------------------------------------

In this article, we covered the basics of how to use LangChain. We learned that LangChain is a framework for building LLM applications that relies on two key factors. The first factor is using outside data, such as a text document. The second factor is using other tools, such as a vector database.

We covered two examples.

First, we looked at one of the classic examples of LangChain - how to chain together multiple questions.

Then, we looked at a practical way to use LangChain to inject domain knowledge by combining it with a vector database like Milvus to query documents.
    